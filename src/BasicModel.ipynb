{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from IPython.core.display import display\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Bidirectional, LSTM, Dense, Activation, Embedding\n",
    "from keras.models import Sequential\n",
    "from process import *\n",
    "\n",
    "glove_len = 100\n",
    "glove = Path(f'../glove.6B.{glove_len}d.txt')\n",
    "docfile = Path('../tac_docria/en/eng.2015.train.pickle')\n",
    "embed = load_glove(glove)\n",
    "\n",
    "@pickled\n",
    "def read_and_extract(path, fun):\n",
    "    with DocumentIO.read(path) as doc:\n",
    "        return fun(list(doc))\n",
    "    \n",
    "def get_core_nlp(docs, lang):\n",
    "    def call_api(doc):\n",
    "        text = str(doc.texts['main'])\n",
    "        return langforia(text, lang).split('\\n')\n",
    "    return [call_api(doc) for doc in docs], docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_label_idx(doc):\n",
    "    doc_index = {}\n",
    "    labels, types = set(), set()\n",
    "    longest = None\n",
    "    for node in doc.layers['tac/entity/gold']:\n",
    "        labels.add(node.fld.label)\n",
    "        types.add(node.fld.type)\n",
    "        entity = node.fld.text\n",
    "        # ignore xml-only entities\n",
    "        if entity:\n",
    "            span = (entity.start, entity.stop)\n",
    "            # TODO: simplify logic if possible\n",
    "            if longest:\n",
    "                # there is a greater span with same start\n",
    "                if span[0] == longest[0] and span[1] > longest[1]:\n",
    "                    del doc_index[longest]\n",
    "                    longest = span\n",
    "                # we are either inside or after the current span\n",
    "                if span[0] > longest[0]:\n",
    "                    # this should never happen (overlapping spans)\n",
    "                    if span[0] <= longest[1] and span[1] > longest[1]:\n",
    "                        continue\n",
    "                        raise ValueError(\"Span %s and Span %s are overlapping\"\n",
    "                                        % (longest, span))\n",
    "                    # we are inside the span\n",
    "                    if span[1] <= longest[1]:\n",
    "                        continue\n",
    "                    # we have a new span\n",
    "                    else:\n",
    "                        longest = span\n",
    "            else:\n",
    "                longest = span\n",
    "            doc_index[span] = (node.fld.type, node.fld.label)\n",
    "    return doc_index, labels, types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_core_nlp(core_nlp, docs):\n",
    "    train, gold = [], []\n",
    "    lbl_sets = defaultdict(set)\n",
    "    labels, types = set(), set()\n",
    "\n",
    "    def add(features, name):\n",
    "        lbl_sets[name].add(features[name])\n",
    "        \n",
    "    for c, d in zip(core_nlp, docs):\n",
    "        gold_std, doc_labels, doc_types = doc_label_idx(d)\n",
    "        labels |= doc_labels\n",
    "        types |= doc_types\n",
    "        def get_entity(span):\n",
    "            return gold_std.get(span, \"O\")\n",
    "        \n",
    "        itr = iter(c)\n",
    "        head = next(itr).split('\\t')[1:]\n",
    "        sentences = [[]]\n",
    "        spans = [[]]\n",
    "        inside = \"\"\n",
    "        for row in itr:\n",
    "            if row:\n",
    "                cols = row.split('\\t')\n",
    "                features = dict(zip(head, cols[1:]))\n",
    "                ne = features['ne']\n",
    "                if inside:\n",
    "                    if ne == ')':\n",
    "                        ne = 'E-' + inside\n",
    "                        inside = ''\n",
    "                    else:\n",
    "                        ne = 'I-' + inside\n",
    "                elif ne[-1] == ')':\n",
    "                    ne = 'S-' + ne[1:-1]\n",
    "                    inside = ''\n",
    "                else:\n",
    "                    inside = ne[1:]\n",
    "                    ne = 'B-' + inside\n",
    "                features['ne'] = ne\n",
    "                add(features, 'pos')\n",
    "                add(features, 'ne')\n",
    "                sentences[-1].append(features)\n",
    "                spans[-1].append((features['start'], features['end']))\n",
    "            else:\n",
    "                sentences.append([])\n",
    "                spans.append([])\n",
    "        if not sentences[-1]:\n",
    "            sentences.pop(-1)\n",
    "            spans.pop(-1)\n",
    "        train.extend(sentences)\n",
    "        \n",
    "        entities = [[get_entity(tuple(map(int,span))) for span in sentence] for sentence in spans]\n",
    "        gold.extend(entities)\n",
    "    out_categories = {pair: index for index, pair in enumerate(product(types, labels), 1)}\n",
    "    out_categories['O'] = 0\n",
    "    return train, lbl_sets, gold, out_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.36081542,  0.44203668,  0.22202288, ...,  0.57384442,\n",
       "          0.36780209,  0.33160592],\n",
       "        [ 0.31121769,  0.10917327,  0.42184412, ...,  0.00628286,\n",
       "          0.90816114,  0.00834343],\n",
       "        [ 0.19557001,  0.15968999, -0.35591999, ...,  0.098046  ,\n",
       "         -0.44168001, -0.17889   ],\n",
       "        ...,\n",
       "        [-0.28209001, -0.47914001, -0.58642   , ...,  0.67146999,\n",
       "         -0.741     , -0.41417   ],\n",
       "        [-0.24793001, -0.6322    ,  0.017061  , ...,  0.20935   ,\n",
       "         -0.32936999, -0.26133999],\n",
       "        [ 0.34773001,  0.17393   ,  0.032938  , ..., -0.14029001,\n",
       "          0.29622   , -0.66772997]]), 406034)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def inverted(a):\n",
    "    return {v:k for k,v in a.items()}\n",
    "\n",
    "def build_indices(train, gold, glove):\n",
    "    wordset = set([word[\"form\"] for sentence in train for word in sentence])\n",
    "    wordset.update(glove.keys())\n",
    "    word_ind = dict(enumerate(wordset, 2))\n",
    "    return word_ind\n",
    "\n",
    "def build_emb_mat(glove, invind):\n",
    "    size = len(invind)+2\n",
    "    mat = np.random.rand(size, glove_len)\n",
    "    for k,v in glove.items():\n",
    "        mat[invind[k], :] = v\n",
    "    return mat, size\n",
    "\n",
    "\n",
    "core_nlp, docs = read_and_extract(docfile, lambda docs: get_core_nlp(docs, 'en'))\n",
    "core_nlp_test, docs_test = read_and_extract(docfile, lambda docs: get_core_nlp(docs, 'en'))\n",
    "train, lbl_sets, gold, out_categories = extract_core_nlp(core_nlp, docs)\n",
    "\n",
    "word_index = build_indices(train, gold, embed)\n",
    "pos_index = dict(enumerate(lbl_sets['pos']))\n",
    "ne_index = dict(enumerate(lbl_sets['ne']))\n",
    "pos_inv = inverted(pos_index)\n",
    "ne_inv = inverted(ne_index)\n",
    "out_index = inverted(out_categories)\n",
    "word_inv = inverted(word_index)\n",
    "\n",
    "build_emb_mat(embed, word_inv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def build_sequence(l, invind, default=None):\n",
    "    if default:\n",
    "        return [invind.get(w, default) for w in l]\n",
    "    return [invind[w] for w in l]\n",
    "\n",
    "def conll_to_word(sentence):\n",
    "    return [word['form'] for word in sentence]\n",
    "\n",
    "x = pad_sequences([build_sequence(conll_to_word(sentence), word_inv, 1) for sentence in train])\n",
    "y = pad_sequences([build_sequence(sentence, out_categories) for sentence in gold])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(embed):\n",
    "    embedding_matrix, mat_size = build_emb_mat(embed, word_inv)\n",
    "    width = len(word_inv) + 2\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(width,\n",
    "                               glove_len,\n",
    "                               mask_zero=True,\n",
    "                               input_length=None))\n",
    "    \n",
    "    model.layers[0].set_weights([embedding_matrix])\n",
    "    model.layers[0].trainable = True\n",
    "    \n",
    "    model.add(Bidirectional(LSTM(25, return_sequences=True), input_shape=(None, width)))\n",
    "    model.add(Dense(len(out_categories), activation='softmax'))\n",
    "    model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='nadam', metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 100)         40603400  \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, None, 50)          25200     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, None, 13)          663       \n",
      "=================================================================\n",
      "Total params: 40,629,263\n",
      "Trainable params: 40,629,263\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "9886/9886 [==============================] - 58s 6ms/step - loss: 0.3385 - acc: 0.9408\n",
      "Epoch 2/10\n",
      "9886/9886 [==============================] - 56s 6ms/step - loss: 0.1235 - acc: 0.9623\n",
      "Epoch 3/10\n",
      "9886/9886 [==============================] - 60s 6ms/step - loss: 0.0789 - acc: 0.9769\n",
      "Epoch 4/10\n",
      "9886/9886 [==============================] - 64s 6ms/step - loss: 0.0568 - acc: 0.9833\n",
      "Epoch 5/10\n",
      "9886/9886 [==============================] - 64s 6ms/step - loss: 0.0441 - acc: 0.9869\n",
      "Epoch 6/10\n",
      "9886/9886 [==============================] - 64s 6ms/step - loss: 0.0358 - acc: 0.9893\n",
      "Epoch 7/10\n",
      "9886/9886 [==============================] - 64s 6ms/step - loss: 0.0299 - acc: 0.9909\n",
      "Epoch 8/10\n",
      "9886/9886 [==============================] - 63s 6ms/step - loss: 0.0252 - acc: 0.9924\n",
      "Epoch 9/10\n",
      "9886/9886 [==============================] - 63s 6ms/step - loss: 0.0214 - acc: 0.9936\n",
      "Epoch 10/10\n",
      "9886/9886 [==============================] - 63s 6ms/step - loss: 0.0182 - acc: 0.9946\n"
     ]
    }
   ],
   "source": [
    "def make_model(x, y, embed, epochs=10, batch_size=128):\n",
    "    y = to_categorical(y, num_classes=len(out_categories))\n",
    "    model = build_model(embed)\n",
    "    model.fit(x, y, epochs=epochs, batch_size=batch_size)\n",
    "    return model\n",
    "\n",
    "model = make_model(x, y, embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import dump\n",
    "with Path('../model.pickle').open('w+b') as f:\n",
    "    dump((model,word_index,out_index), f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
