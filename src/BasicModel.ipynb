{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from IPython.core.display import display\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Bidirectional, LSTM, Dense, Activation, Embedding, Concatenate, Input\n",
    "from keras.models import Model\n",
    "from process import *\n",
    "\n",
    "glove_len = 100\n",
    "glove = Path(f'../glove.6B.{glove_len}d.txt')\n",
    "docfile = Path('../corpus/tac/lang/en/eng.2015.train.pickle')\n",
    "embed = load_glove(glove)\n",
    "\n",
    "@pickled\n",
    "def read_and_extract(path, fun):\n",
    "    with DocumentIO.read(path) as doc:\n",
    "        return fun(list(doc)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverted(a):\n",
    "    return {v:k for k,v in a.items()}\n",
    "\n",
    "def build_indices(train, gold, glove):\n",
    "    wordset = set([word[\"form\"] for sentence in train for word in sentence])\n",
    "    wordset.update(glove.keys())\n",
    "    word_ind = dict(enumerate(wordset, 2))\n",
    "    return word_ind\n",
    "\n",
    "def emb_mat_init(glove, invind):\n",
    "    def initializer(shape, dtype=None):\n",
    "        mat = np.random.random_sample(shape)\n",
    "        for k,v in glove.items():\n",
    "            mat[invind[k], :] = v\n",
    "        return mat\n",
    "    return initializer\n",
    "\n",
    "\n",
    "core_nlp, docs = read_and_extract(docfile, lambda docs: get_core_nlp(docs, 'en'))\n",
    "core_nlp_test, docs_test = read_and_extract(docfile, lambda docs: get_core_nlp(docs, 'en'))\n",
    "train, lbl_sets, gold, out_categories, _, _ = docria_extract(core_nlp, docs)\n",
    "\n",
    "word_index = build_indices(train, gold, embed)\n",
    "pos_index = dict(enumerate(lbl_sets['pos']))\n",
    "ne_index = dict(enumerate(lbl_sets['ne']))\n",
    "pos_inv = inverted(pos_index)\n",
    "ne_inv = inverted(ne_index)\n",
    "out_index = inverted(out_categories)\n",
    "word_inv = inverted(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def build_sequence(l, invind, default=None):\n",
    "    if default:\n",
    "        return [invind.get(w, default) for w in l]\n",
    "    return [invind[w] for w in l]\n",
    "\n",
    "def mapget(key, seq):\n",
    "    return (collection[key] for collection in seq)\n",
    "\n",
    "def conll_to_word(sentence):\n",
    "    return [word['form'] for word in sentence]\n",
    "\n",
    "def to_categories(data, key, inv, default=None, categorical=True):\n",
    "    fields = (mapget(key, sentence) for sentence in data)\n",
    "    cat_seq = [build_sequence(f, inv, default=default) for f in fields]\n",
    "    padded = pad_sequences(cat_seq)\n",
    "    if categorical:\n",
    "        return to_categorical(padded)\n",
    "    return padded\n",
    "    \n",
    "x_word = to_categories(train, 'form', word_inv, default=1, categorical=False)\n",
    "x_pos = to_categories(train, 'pos', pos_inv)\n",
    "x_ne = to_categories(train, 'ne', ne_inv)\n",
    "y = pad_sequences(gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(max_len, embed, npos, nne):\n",
    "    width = len(word_inv) + 2\n",
    "    pos = Input(shape=(max_len, npos))\n",
    "    ne = Input(shape=(max_len, nne))\n",
    "    form = Input(shape=(max_len,))\n",
    "    emb = Embedding(width,\n",
    "                    glove_len,\n",
    "                    embeddings_initializer=emb_mat_init(embed, word_inv),\n",
    "                    mask_zero=True,\n",
    "                    input_length=None)(form)\n",
    "    \n",
    "    emb.trainable = True\n",
    "    \n",
    "    concat = Concatenate()([emb, pos, ne])\n",
    "    \n",
    "    lstm = Bidirectional(LSTM(25, return_sequences=True), input_shape=(None, width))(concat)\n",
    "    out = Dense(len(out_categories), activation='softmax')(lstm)\n",
    "    model = Model(inputs= [form, pos, ne], outputs=out)\n",
    "    #model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='nadam', metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9886, 223, 50)\n"
     ]
    }
   ],
   "source": [
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9886, 223)\n",
      "Epoch 1/10\n",
      "9886/9886 [==============================] - 63s 6ms/step - loss: 0.7180 - acc: 0.9100\n",
      "Epoch 2/10\n",
      "9886/9886 [==============================] - 55s 6ms/step - loss: 0.2322 - acc: 0.9427\n",
      "Epoch 3/10\n",
      "9886/9886 [==============================] - 66s 7ms/step - loss: 0.1502 - acc: 0.9656\n",
      "Epoch 4/10\n",
      "9886/9886 [==============================] - 71s 7ms/step - loss: 0.1106 - acc: 0.9767\n",
      "Epoch 5/10\n",
      "9886/9886 [==============================] - 72s 7ms/step - loss: 0.0898 - acc: 0.9795\n",
      "Epoch 6/10\n",
      "9886/9886 [==============================] - 71s 7ms/step - loss: 0.0765 - acc: 0.9817\n",
      "Epoch 7/10\n",
      "9886/9886 [==============================] - 71s 7ms/step - loss: 0.0667 - acc: 0.9837\n",
      "Epoch 8/10\n",
      "9886/9886 [==============================] - 71s 7ms/step - loss: 0.0591 - acc: 0.9851\n",
      "Epoch 9/10\n",
      "9886/9886 [==============================] - 70s 7ms/step - loss: 0.0526 - acc: 0.9865\n",
      "Epoch 10/10\n",
      "9886/9886 [==============================] - 70s 7ms/step - loss: 0.0471 - acc: 0.9878\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 223)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 223, 100)     40160200    input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            (None, 223, 45)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 223, 23)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 223, 168)     0           embedding_1[0][0]                \n",
      "                                                                 input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 223, 50)      38800       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 223, 50)      2550        bidirectional_1[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 40,201,550\n",
      "Trainable params: 40,201,550\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def make_model(max_len, x, y, embed, npos, nne, epochs=3, batch_size=128):\n",
    "    #y = to_categorical(y, num_classes=len(out_categories))\n",
    "    model = build_model(max_len, embed, npos, nne)\n",
    "    model.fit(x, y, epochs=epochs, batch_size=batch_size)\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "print(x_word.shape)\n",
    "model = make_model(x_word.shape[1], [x_word, x_pos, x_ne], y, embed, len(pos_inv), len(ne_inv), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test, _, gold_test, _, _, _ = docria_extract(core_nlp_test, docs_test)\n",
    "x_word_test = to_categories(test, 'form', word_inv, default=1, categorical=False)\n",
    "x_pos_test = to_categories(test, 'pos', pos_inv)\n",
    "x_ne_test = to_categories(test, 'ne', ne_inv)\n",
    "y_test = pad_sequences(gold)\n",
    "pred = model.predict([x_word_test, x_pos_test, x_ne_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('O', 'NOE', 'OUT') 0.9968802271326701 181744\n",
      "('B', 'NAM', 'PER') 0.9769230769230769 1170\n",
      "('E', 'NAM', 'PER') 0.9845626072041166 1166\n",
      "('S', 'NOM', 'PER') 0.7955625990491284 1262\n",
      "('S', 'NAM', 'PER') 0.9800435413642961 2756\n",
      "('S', 'NAM', 'GPE') 0.9831223628691983 2844\n",
      "('S', 'NAM', 'ORG') 0.9250180245133381 1387\n",
      "('B', 'NAM', 'ORG') 0.7574692442882249 569\n",
      "('I', 'NAM', 'ORG') 0.7134328358208956 335\n",
      "('E', 'NAM', 'ORG') 0.8068181818181818 616\n",
      "('B', 'NAM', 'FAC') 0.44660194174757284 103\n",
      "('E', 'NAM', 'FAC') 0.5583333333333333 120\n",
      "('I', 'NAM', 'PER') 0.719626168224299 107\n",
      "('B', 'NAM', 'GPE') 0.8421052631578947 209\n",
      "('E', 'NAM', 'GPE') 0.8957345971563981 211\n",
      "('B', 'NAM', 'LOC') 0.6454545454545455 110\n",
      "('E', 'NAM', 'LOC') 0.4424778761061947 113\n",
      "('S', 'NAM', 'LOC') 0.5989010989010989 182\n",
      "('I', 'NAM', 'FAC') 0.0 39\n",
      "('B', 'NOM', 'PER') 0.0 79\n",
      "('E', 'NOM', 'PER') 0.0 80\n",
      "('S', 'NAM', 'FAC') 0.0 52\n",
      "('I', 'NAM', 'LOC') 0.0 11\n",
      "('S', 'NAM', 'TTL') 0.0 8\n",
      "('I', 'NOM', 'PER') 0.0 22\n",
      "('I', 'NAM', 'GPE') 0.0 25\n",
      "('B', 'NAM', 'TTL') 0.0 3\n",
      "('E', 'NAM', 'TTL') 0.0 3\n",
      "('I', 'NAM', 'TTL') 0.0 9\n",
      "0.884923846663233\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "def zip_from_end(a, b):\n",
    "    shortest = min(len(a), len(b))\n",
    "    return ((a[i], b[i]) for i in range(-shortest, 0))\n",
    "\n",
    "actual = Counter()\n",
    "correct = Counter()\n",
    "for p, g in zip(pred, gold_test):\n",
    "    for a,b in zip_from_end(p, g):\n",
    "        actual_tag = out_index[np.argmax(b)]\n",
    "        actual[actual_tag] += 1\n",
    "        if np.argmax(a) == np.argmax(b):\n",
    "            correct[actual_tag] += 1\n",
    "\n",
    "for k in actual:\n",
    "    print(k, correct[k]/actual[k], actual[k])\n",
    "    \n",
    "corr_sum = sum(correct[k] for k in correct if k != ('O', 'NOE', 'OUT'))\n",
    "act_sum = sum(actual[k] for k in actual if k != ('O', 'NOE', 'OUT'))\n",
    "print(corr_sum/act_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pickle import dump\n",
    "#with Path('../model.pickle').open('w+b') as f:\n",
    "    #dump((model,word_index,out_index), f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
